{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submarine Sonar: Mine vs Rock Classification\n",
    "\n",
    "## Can We Teach a Machine to Hear the Difference Between Death and a Pebble?\n",
    "\n",
    "---\n",
    "\n",
    "**Alright, let's dive in (pun absolutely intended).**\n",
    "\n",
    "So here's the deal - sonar is basically underwater echolocation. Submarines send out sound waves, those waves bounce off stuff, and the pattern of echoes tells you what's out there. Simple concept, terrifying stakes.\n",
    "\n",
    "**What we're working with:**\n",
    "- **208 sonar returns** - yeah, it's small, but it's a classic\n",
    "- **60 frequency bands** - each one measures how much energy bounced back at that frequency (0.0 to 1.0)\n",
    "- **The signal:** A frequency-modulated chirp - starts low, goes high\n",
    "- **Two targets:** Rocks (R) and Mines (M) - metal cylinders that go BOOM\n",
    "- **Different angles:** 90 degrees coverage for mines, 180 for rocks\n",
    "\n",
    "**The question:** Can we look at 60 numbers and figure out if the submarine should chill or panic?\n",
    "\n",
    "Let's find out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up Shop\n",
    "\n",
    "First things first - import the tools, load the data, see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual suspects\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# The model army\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                            accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, roc_curve)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"All systems loaded. Ready to ping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened:** We loaded pandas for data handling, numpy for math, matplotlib/seaborn for visuals, scipy for statistics, and a whole army of sklearn models. Think of it as assembling the Avengers, but for data science.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('Sonar.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nThat's {df.shape[0]} sonar readings, each with {df.shape[1]-1} frequency measurements + 1 label\")\n",
    "print(f\"\\nColumns: {df.shape[1]-1} frequency bands (Freq_1 to Freq_60) + Label (M/R)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this tells us:** 208 rows, 61 columns. Each row is one sonar ping bouncing off something. Each of the 60 frequency columns tells us how much energy came back at that specific frequency. The last column tells us what the object actually was - Mine or Rock.\n",
    "\n",
    "208 samples is tiny by modern standards, but this dataset is from 1988 and it's still used today because it's genuinely challenging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading these numbers:** Each cell is an energy value between 0 and 1. Higher = more energy bounced back at that frequency. The 'Label' column shows R (Rock) or M (Mine). \n",
    "\n",
    "Notice how the values vary a lot - some frequencies get strong returns (0.8+), others barely register (0.01). That variation is where the signal hides.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic EDA - Getting to Know Our Data\n",
    "\n",
    "Before we throw algorithms at this, let's actually understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution - how many mines vs rocks?\n",
    "print(\"TARGET DISTRIBUTION\")\n",
    "print(\"=\"*40)\n",
    "target_counts = df['Label'].value_counts()\n",
    "print(f\"\\nMines (M):  {target_counts['M']} ({target_counts['M']/len(df)*100:.1f}%)\")\n",
    "print(f\"Rocks (R):  {target_counts['R']} ({target_counts['R']/len(df)*100:.1f}%)\")\n",
    "print(f\"\\nTotal: {len(df)} samples\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#e74c3c', '#3498db']\n",
    "bars = ax.bar(['Mine (M)\\nDANGER', 'Rock (R)\\nSafe'], \n",
    "              [target_counts['M'], target_counts['R']], \n",
    "              color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('What Are We Classifying?')\n",
    "for bar, count in zip(bars, [target_counts['M'], target_counts['R']]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "            str(count), ha='center', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What this means:** 111 mines, 97 rocks. Pretty balanced - about 53% mines, 47% rocks. This is good news because we don't need to worry about class imbalance messing with our models. \n",
    "\n",
    "If it was like 95% rocks and 5% mines, a lazy model could just predict \"rock\" every time and look accurate while being useless.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of frequency bands\n",
    "print(\"FREQUENCY BAND STATISTICS\")\n",
    "print(\"=\"*40)\n",
    "freq_cols = [col for col in df.columns if col != 'Label']\n",
    "stats_df = df[freq_cols].describe().T\n",
    "stats_df['range'] = stats_df['max'] - stats_df['min']\n",
    "print(f\"\\nNumber of frequency bands: {len(freq_cols)}\")\n",
    "print(f\"\\nOverall value range: {df[freq_cols].min().min():.4f} to {df[freq_cols].max().max():.4f}\")\n",
    "print(f\"\\nSummary across all bands:\")\n",
    "print(stats_df[['mean', 'std', 'min', 'max', 'range']].describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breaking this down:** All values are between 0 and 1 (normalized energy readings). The mean values hover around 0.03-0.3 depending on the frequency band. Standard deviations tell us how much variation exists - more variation = potentially more useful for classification.\n",
    "\n",
    "The 'range' column shows how spread out each feature is. Features with tiny ranges might not be very informative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values check\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\"*40)\n",
    "missing = df.isnull().sum().sum()\n",
    "print(f\"\\nTotal missing values: {missing}\")\n",
    "if missing == 0:\n",
    "    print(\"Clean dataset - no missing data to handle.\")\n",
    "\n",
    "# Duplicates check\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice:** No missing values, no duplicates. This dataset is clean - rare in the wild, but this is a curated benchmark dataset so it makes sense.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The money plot: Average frequency profile for Mines vs Rocks\n",
    "# This is THE key visualization - shows how the two classes differ across frequencies\n",
    "\n",
    "mines = df[df['Label'] == 'M'][freq_cols].mean()\n",
    "rocks = df[df['Label'] == 'R'][freq_cols].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = range(1, 61)\n",
    "ax.plot(x, mines.values, 'r-', linewidth=2, label='Mine (Metal Cylinder)', marker='o', markersize=4)\n",
    "ax.plot(x, rocks.values, 'b-', linewidth=2, label='Rock', marker='s', markersize=4)\n",
    "ax.fill_between(x, mines.values, rocks.values, alpha=0.3, color='gray')\n",
    "\n",
    "ax.set_xlabel('Frequency Band (1-60)')\n",
    "ax.set_ylabel('Mean Energy Response')\n",
    "ax.set_title('Average Sonar Signature: Mine vs Rock\\n(The gap between lines = how distinguishable they are)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(1, 60)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the most important plot in the whole analysis.** It shows the average sonar \"fingerprint\" of mines vs rocks.\n",
    "\n",
    "**What to look for:**\n",
    "- Where the lines are far apart = frequencies that are good at distinguishing mine from rock\n",
    "- Where the lines overlap = frequencies that don't help much\n",
    "- The gray shaded area shows the \"difference zone\" - bigger gaps = easier classification\n",
    "\n",
    "You can see mines and rocks have different energy patterns, especially in the middle frequency bands. Metal reflects sonar differently than stone - that's the physics we're exploiting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of energy across all frequency bands\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Overall distribution\n",
    "all_values = df[freq_cols].values.flatten()\n",
    "axes[0, 0].hist(all_values, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Energy Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of All Energy Readings')\n",
    "axes[0, 0].axvline(x=np.mean(all_values), color='red', linestyle='--', label=f'Mean: {np.mean(all_values):.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Mean energy per sample by class\n",
    "df['mean_energy'] = df[freq_cols].mean(axis=1)\n",
    "mines_energy = df[df['Label'] == 'M']['mean_energy']\n",
    "rocks_energy = df[df['Label'] == 'R']['mean_energy']\n",
    "\n",
    "axes[0, 1].hist(mines_energy, bins=20, alpha=0.6, color='red', label='Mines', edgecolor='black')\n",
    "axes[0, 1].hist(rocks_energy, bins=20, alpha=0.6, color='blue', label='Rocks', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Mean Energy per Sample')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Mean Energy Distribution by Class')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Variance per sample by class\n",
    "df['var_energy'] = df[freq_cols].var(axis=1)\n",
    "mines_var = df[df['Label'] == 'M']['var_energy']\n",
    "rocks_var = df[df['Label'] == 'R']['var_energy']\n",
    "\n",
    "axes[1, 0].hist(mines_var, bins=20, alpha=0.6, color='red', label='Mines', edgecolor='black')\n",
    "axes[1, 0].hist(rocks_var, bins=20, alpha=0.6, color='blue', label='Rocks', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Energy Variance per Sample')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Energy Variance Distribution by Class')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Box plot of selected frequencies\n",
    "selected_freqs = ['Freq_10', 'Freq_20', 'Freq_30', 'Freq_40', 'Freq_50']\n",
    "df_melt = df[selected_freqs + ['Label']].melt(id_vars='Label', var_name='Frequency', value_name='Energy')\n",
    "sns.boxplot(data=df_melt, x='Frequency', y='Energy', hue='Label', palette=['red', 'blue'], ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Energy Distribution at Selected Frequencies')\n",
    "axes[1, 1].legend(title='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temp columns\n",
    "df.drop(['mean_energy', 'var_energy'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Four plots, four insights:**\n",
    "\n",
    "1. **Top-left (Overall distribution):** Most energy readings are low (0.0-0.2), with a right-skewed distribution. High energy returns are less common.\n",
    "\n",
    "2. **Top-right (Mean energy by class):** Mines and rocks have overlapping mean energy distributions - you can't just say \"high energy = mine\". It's more nuanced than that.\n",
    "\n",
    "3. **Bottom-left (Variance by class):** Similar story - variance alone doesn't cleanly separate the classes. We need the full 60-dimensional pattern.\n",
    "\n",
    "4. **Bottom-right (Box plots):** At different frequencies, the separation varies. Some frequencies (like Freq_10, Freq_20) show clear differences between classes, others don't.\n",
    "\n",
    "**Takeaway:** This isn't a simple problem. The classes are entangled, and we'll need sophisticated models that can learn complex patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap - how do frequency bands relate to each other?\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "corr_matrix = df[freq_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Upper triangle mask\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='RdBu_r', center=0, \n",
    "            square=True, linewidths=0.5, ax=ax,\n",
    "            cbar_kws={'label': 'Correlation', 'shrink': 0.8})\n",
    "ax.set_title('Correlation Between Frequency Bands\\n(Red = positive correlation, Blue = negative)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "high_corr = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "print(f\"\\nHighly correlated pairs (|r| > 0.9): {len(high_corr)}\")\n",
    "if len(high_corr) > 0:\n",
    "    print(\"\\nTop 5:\")\n",
    "    for f1, f2, r in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
    "        print(f\"  {f1} <-> {f2}: {r:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What the correlation heatmap reveals:**\n",
    "\n",
    "- **Adjacent frequencies are correlated** (the diagonal red band) - makes physical sense, neighboring frequency bands behave similarly\n",
    "- **Some frequency bands are almost redundant** - if two features are 95% correlated, one of them isn't adding much new information\n",
    "- **Distant frequencies are less correlated** - low frequencies and high frequencies capture different aspects of the signal\n",
    "\n",
    "This explains why dimensionality reduction (PCA) might work well here - there's redundancy we can compress.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Statistical Study\n",
    "\n",
    "Time to get serious. Which frequencies actually matter statistically for telling mines from rocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests: T-test and Mann-Whitney U for each frequency band\n",
    "# T-test assumes normal distribution, Mann-Whitney doesn't\n",
    "\n",
    "mines_data = df[df['Label'] == 'M'][freq_cols]\n",
    "rocks_data = df[df['Label'] == 'R'][freq_cols]\n",
    "\n",
    "stat_results = []\n",
    "\n",
    "for col in freq_cols:\n",
    "    mine_vals = mines_data[col]\n",
    "    rock_vals = rocks_data[col]\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, t_pval = stats.ttest_ind(mine_vals, rock_vals)\n",
    "    \n",
    "    # Mann-Whitney U (non-parametric)\n",
    "    u_stat, u_pval = stats.mannwhitneyu(mine_vals, rock_vals, alternative='two-sided')\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((mine_vals.std()**2 + rock_vals.std()**2) / 2)\n",
    "    cohens_d = (mine_vals.mean() - rock_vals.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    stat_results.append({\n",
    "        'Feature': col,\n",
    "        'Mine_Mean': mine_vals.mean(),\n",
    "        'Rock_Mean': rock_vals.mean(),\n",
    "        'Diff': mine_vals.mean() - rock_vals.mean(),\n",
    "        'T_Statistic': t_stat,\n",
    "        'T_PValue': t_pval,\n",
    "        'MW_PValue': u_pval,\n",
    "        'Cohens_d': cohens_d,\n",
    "        'Significant': 'Yes' if t_pval < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "stat_df = pd.DataFrame(stat_results)\n",
    "stat_df = stat_df.sort_values('T_PValue')\n",
    "\n",
    "print(\"STATISTICAL SIGNIFICANCE TEST RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSignificant features (p < 0.05): {(stat_df['Significant'] == 'Yes').sum()}/{len(stat_df)}\")\n",
    "print(f\"\\nTop 10 Most Significant Features:\")\n",
    "print(stat_df[['Feature', 'Mine_Mean', 'Rock_Mean', 'Diff', 'T_PValue', 'Cohens_d', 'Significant']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What the stats tell us:**\n",
    "\n",
    "- **T-test p-value < 0.05** = The difference between mine and rock means is statistically significant (unlikely to be random chance)\n",
    "- **Cohen's d** = Effect size. How BIG is the difference?\n",
    "  - |d| < 0.2 = tiny difference\n",
    "  - |d| ~ 0.5 = medium difference  \n",
    "  - |d| > 0.8 = large difference\n",
    "\n",
    "The features at the top of this list are your best discriminators - they show the biggest, most reliable differences between mines and rocks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize statistical significance across all frequencies\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# P-values across frequencies\n",
    "stat_df_sorted = stat_df.sort_values('Feature', key=lambda x: x.str.extract('(\\d+)')[0].astype(int))\n",
    "freq_nums = [int(f.split('_')[1]) for f in stat_df_sorted['Feature']]\n",
    "\n",
    "colors = ['green' if p < 0.05 else 'red' for p in stat_df_sorted['T_PValue']]\n",
    "axes[0].bar(freq_nums, -np.log10(stat_df_sorted['T_PValue']), color=colors, edgecolor='black', alpha=0.7)\n",
    "axes[0].axhline(y=-np.log10(0.05), color='red', linestyle='--', linewidth=2, label='p=0.05 threshold')\n",
    "axes[0].set_xlabel('Frequency Band')\n",
    "axes[0].set_ylabel('-log10(p-value)')\n",
    "axes[0].set_title('Statistical Significance by Frequency Band\\n(Higher = more significant, Green = significant)')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 61)\n",
    "\n",
    "# Effect size (Cohen's d) across frequencies\n",
    "colors_d = ['#e74c3c' if d > 0 else '#3498db' for d in stat_df_sorted['Cohens_d']]\n",
    "axes[1].bar(freq_nums, stat_df_sorted['Cohens_d'], color=colors_d, edgecolor='black', alpha=0.7)\n",
    "axes[1].axhline(y=0.5, color='green', linestyle='--', alpha=0.7, label='Medium effect (0.5)')\n",
    "axes[1].axhline(y=-0.5, color='green', linestyle='--', alpha=0.7)\n",
    "axes[1].axhline(y=0.8, color='orange', linestyle='--', alpha=0.7, label='Large effect (0.8)')\n",
    "axes[1].axhline(y=-0.8, color='orange', linestyle='--', alpha=0.7)\n",
    "axes[1].set_xlabel('Frequency Band')\n",
    "axes[1].set_ylabel(\"Cohen's d (Effect Size)\")\n",
    "axes[1].set_title(\"Effect Size by Frequency Band\\n(Red = Mines higher, Blue = Rocks higher)\")\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_xlim(0, 61)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading these plots:**\n",
    "\n",
    "**Top plot (Significance):**\n",
    "- Bars above the red line = statistically significant differences\n",
    "- Higher bars = more confident the difference is real\n",
    "- Middle frequencies tend to be more discriminative\n",
    "\n",
    "**Bottom plot (Effect Size):**\n",
    "- Red bars = Mines have higher energy at that frequency\n",
    "- Blue bars = Rocks have higher energy at that frequency\n",
    "- Taller bars = bigger practical difference\n",
    "- Bars beyond the green/orange lines = medium/large effects worth paying attention to\n",
    "\n",
    "**Pattern emerging:** The signal isn't uniform across frequencies. Some bands are diagnostic goldmines, others are noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nFeatures with p < 0.05:  {(stat_df['T_PValue'] < 0.05).sum()}\")\n",
    "print(f\"Features with p < 0.01:  {(stat_df['T_PValue'] < 0.01).sum()}\")\n",
    "print(f\"Features with p < 0.001: {(stat_df['T_PValue'] < 0.001).sum()}\")\n",
    "print(f\"\\nFeatures with |Cohen's d| > 0.5 (medium effect): {(abs(stat_df['Cohens_d']) > 0.5).sum()}\")\n",
    "print(f\"Features with |Cohen's d| > 0.8 (large effect):  {(abs(stat_df['Cohens_d']) > 0.8).sum()}\")\n",
    "\n",
    "# Top discriminative features\n",
    "print(f\"\\n\\nTOP 5 MOST DISCRIMINATIVE FEATURES:\")\n",
    "print(\"-\"*50)\n",
    "top5 = stat_df.nlargest(5, 'Cohens_d', key=abs)\n",
    "for _, row in top5.iterrows():\n",
    "    direction = \"Mines > Rocks\" if row['Cohens_d'] > 0 else \"Rocks > Mines\"\n",
    "    print(f\"{row['Feature']:10s} | Cohen's d: {row['Cohens_d']:+.3f} | {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The statistical verdict:** A good chunk of frequency bands show significant differences between mines and rocks, but the effect sizes are mostly small to medium. This isn't a slam-dunk separation - it's a subtle pattern recognition problem.\n",
    "\n",
    "The top discriminative features tell us which frequencies our models should pay most attention to. Spoiler: Random Forest will figure this out automatically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dimensionality Reduction\n",
    "\n",
    "60 features is a lot. Can we compress this information while keeping what matters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for dimensionality reduction\n",
    "X = df[freq_cols].values\n",
    "y = df['Label'].values\n",
    "\n",
    "# Standardize (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original shape: {X.shape}\")\n",
    "print(f\"Scaled shape: {X_scaled.shape}\")\n",
    "print(f\"\\nScaled data mean: {X_scaled.mean():.6f} (should be ~0)\")\n",
    "print(f\"Scaled data std: {X_scaled.std():.6f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why scale?** PCA is sensitive to the scale of features. If one feature ranges from 0-1000 and another from 0-1, PCA would think the first one is more important just because its numbers are bigger. Standardizing puts everyone on equal footing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - Principal Component Analysis\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Explained variance\n",
    "exp_var = pca_full.explained_variance_ratio_\n",
    "cum_var = np.cumsum(exp_var)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0].bar(range(1, 21), exp_var[:20], color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Variance Explained by Each Component (Top 20)')\n",
    "axes[0].set_xticks(range(1, 21))\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, 61), cum_var, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].axhline(y=0.90, color='red', linestyle='--', label='90% variance')\n",
    "axes[1].axhline(y=0.95, color='orange', linestyle='--', label='95% variance')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('How Many Components to Keep?')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(1, 60)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find components needed for thresholds\n",
    "n_90 = np.argmax(cum_var >= 0.90) + 1\n",
    "n_95 = np.argmax(cum_var >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nComponents needed for 90% variance: {n_90}\")\n",
    "print(f\"Components needed for 95% variance: {n_95}\")\n",
    "print(f\"\\nFirst 3 components explain: {cum_var[2]*100:.1f}% of variance\")\n",
    "print(f\"First 10 components explain: {cum_var[9]*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What PCA revealed:**\n",
    "\n",
    "- The first few components capture most of the variance (the \"important stuff\")\n",
    "- We can go from 60 features down to ~15-20 while keeping 90-95% of the information\n",
    "- This confirms our earlier observation: there's redundancy in the 60 frequency bands\n",
    "\n",
    "**The practical implication:** Simpler models might work fine if we reduce dimensions first. Less overfitting risk with fewer features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data in 2D using PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# PCA 2D projection\n",
    "colors = ['red' if label == 'M' else 'blue' for label in y]\n",
    "axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=colors, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "axes[0].set_title('PCA: 60D compressed to 2D')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='red', edgecolor='black', label='Mine'),\n",
    "                   Patch(facecolor='blue', edgecolor='black', label='Rock')]\n",
    "axes[0].legend(handles=legend_elements)\n",
    "\n",
    "# t-SNE 2D projection (better for visualization, preserves local structure)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=colors, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "axes[1].set_title('t-SNE: Nonlinear 2D Projection')\n",
    "axes[1].legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two ways to see 60 dimensions in 2D:**\n",
    "\n",
    "**PCA (left):** Linear projection. Fast, interpretable. Shows the \"main directions\" of variation. Notice the classes overlap quite a bit - this problem isn't linearly separable in 2D.\n",
    "\n",
    "**t-SNE (right):** Non-linear projection. Better at revealing clusters. You might see slightly better separation, but still significant overlap.\n",
    "\n",
    "**The overlap is the challenge.** There's no magic boundary that cleanly separates all mines from all rocks. This is why this dataset has been a benchmark for decades - it's genuinely hard.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D PCA visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "mines_mask = y == 'M'\n",
    "rocks_mask = y == 'R'\n",
    "\n",
    "ax.scatter(X_pca_3d[mines_mask, 0], X_pca_3d[mines_mask, 1], X_pca_3d[mines_mask, 2], \n",
    "           c='red', label='Mine', alpha=0.6, s=50)\n",
    "ax.scatter(X_pca_3d[rocks_mask, 0], X_pca_3d[rocks_mask, 1], X_pca_3d[rocks_mask, 2], \n",
    "           c='blue', label='Rock', alpha=0.6, s=50)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]*100:.1f}%)')\n",
    "ax.set_title(f'3D PCA Projection\\n(Total: {sum(pca_3d.explained_variance_ratio_)*100:.1f}% variance explained)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D helps a bit:** With three principal components we capture more variance and can see the data from different angles. The classes are still intermingled, but you can start to see regions that are more \"mine-heavy\" vs \"rock-heavy\".\n",
    "\n",
    "**Bottom line on dimensionality reduction:** It's useful for visualization and can help with model performance, but this dataset's fundamental challenge remains - the classes are not easily separable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Classification Battle\n",
    "\n",
    "Alright, this is what we came for. Let's throw some algorithms at this problem and see who survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final data\n",
    "X = df[freq_cols].values\n",
    "y_encoded = LabelEncoder().fit_transform(df['Label'].values)  # M=0, R=1? Let's check\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['Label'].values)\n",
    "print(f\"Label encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup complete:**\n",
    "- 80% for training (166 samples), 20% for testing (42 samples)\n",
    "- Stratified split maintains class proportions in both sets\n",
    "- Data is scaled (important for SVM, KNN, Neural Networks)\n",
    "\n",
    "With only 42 test samples, every single prediction matters. One mistake = ~2.4% accuracy drop.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model army\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'SVM (Linear)': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(f\"Models entering the arena: {len(models)}\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The contenders:**\n",
    "- **Logistic Regression:** Simple, interpretable, often surprisingly good\n",
    "- **Decision Tree:** Easy to understand but prone to overfitting\n",
    "- **Random Forest:** Ensemble of trees, usually robust\n",
    "- **Gradient Boosting:** Sequential tree building, powerful but slow\n",
    "- **KNN:** \"You are who your neighbors are\" - simple but effective\n",
    "- **SVM (RBF & Linear):** Classic for this dataset, finds optimal separating boundary\n",
    "- **Naive Bayes:** Fast and simple, assumes feature independence\n",
    "- **Neural Network:** The deep learning option (not actually deep here, just one hidden layer)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Training and evaluating models...\\n\")\n",
    "print(\"-\"*85)\n",
    "print(f\"{'Model':<25} | {'Test Acc':>8} | {'CV Mean':>8} | {'CV Std':>7} | {'Precision':>9} | {'Recall':>7}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for distance-based models\n",
    "    if name in ['K-Nearest Neighbors', 'SVM (RBF)', 'SVM (Linear)', 'Neural Network', 'Logistic Regression']:\n",
    "        X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    else:\n",
    "        X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    if name in ['K-Nearest Neighbors', 'SVM (RBF)', 'SVM (Linear)', 'Neural Network', 'Logistic Regression']:\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Test_Accuracy': acc,\n",
    "        'CV_Mean': cv_scores.mean(),\n",
    "        'CV_Std': cv_scores.std(),\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1_Score': f1\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<25} | {acc*100:>7.2f}% | {cv_scores.mean()*100:>7.2f}% | {cv_scores.std()*100:>6.2f}% | {prec*100:>8.2f}% | {rec*100:>6.2f}%\")\n",
    "\n",
    "print(\"-\"*85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading these results:**\n",
    "\n",
    "- **Test Accuracy:** Performance on the held-out 42 samples\n",
    "- **CV Mean:** Average accuracy across 5-fold cross-validation (more reliable estimate)\n",
    "- **CV Std:** How much the accuracy varies across folds (lower = more stable)\n",
    "- **Precision:** Of all predicted positives, how many were correct?\n",
    "- **Recall:** Of all actual positives, how many did we catch?\n",
    "\n",
    "**What to look for:** High test accuracy is good, but CV mean is more trustworthy with small datasets. Big gap between test accuracy and CV mean might indicate lucky/unlucky test split.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame and find the champion\n",
    "results_df = pd.DataFrame(results).sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL LEADERBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for rank, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    medal = \"[CHAMPION]\" if rank == 1 else f\"[{rank}]\"\n",
    "    print(f\"{medal:12s} {row['Model']:<25} | Test: {row['Test_Accuracy']*100:.2f}% | CV: {row['CV_Mean']*100:.2f}%\")\n",
    "\n",
    "# Champion\n",
    "champion_name = results_df.iloc[0]['Model']\n",
    "champion_acc = results_df.iloc[0]['Test_Accuracy']\n",
    "print(f\"\\nChampion: {champion_name} with {champion_acc*100:.2f}% test accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The moment of truth.** The leaderboard shows who came out on top. Remember, this isn't a 100%-accuracy dataset - anything above 80% is respectable, above 85% is great, and above 90% is excellent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Test accuracy comparison\n",
    "colors = ['gold' if acc == results_df['Test_Accuracy'].max() else 'steelblue' \n",
    "          for acc in results_df['Test_Accuracy']]\n",
    "bars = axes[0].barh(results_df['Model'], results_df['Test_Accuracy'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Test Set Accuracy')\n",
    "axes[0].set_xlim(0.5, 1.0)\n",
    "for bar, acc in zip(bars, results_df['Test_Accuracy']):\n",
    "    axes[0].text(acc + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{acc*100:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# CV accuracy with error bars\n",
    "axes[1].barh(results_df['Model'], results_df['CV_Mean'], \n",
    "             xerr=results_df['CV_Std'], color='steelblue', edgecolor='black', capsize=5)\n",
    "axes[1].set_xlabel('Accuracy')\n",
    "axes[1].set_title('Cross-Validation Accuracy (with std dev)')\n",
    "axes[1].set_xlim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual comparison:**\n",
    "\n",
    "- **Left plot:** Raw test accuracy - gold bar is the winner\n",
    "- **Right plot:** CV accuracy with error bars - shows how stable each model is\n",
    "\n",
    "Models with small error bars are more reliable - they perform consistently across different data splits. Large error bars mean the model's performance depends heavily on which samples it sees.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Get predictions\n",
    "if best_model_name in ['K-Nearest Neighbors', 'SVM (RBF)', 'SVM (Linear)', 'Neural Network', 'Logistic Regression']:\n",
    "    y_pred_best = best_model.predict(X_test_scaled)\n",
    "    y_prob_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "else:\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    y_prob_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"DETAILED ANALYSIS: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=['Mine (M)', 'Rock (R)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the classification report:**\n",
    "\n",
    "- **Precision for Mines:** When we predict \"Mine\", how often are we right? (Important: false alarms waste resources)\n",
    "- **Recall for Mines:** Of all actual mines, how many did we catch? (CRITICAL: missing a mine = BOOM)\n",
    "- **F1-Score:** Balance between precision and recall\n",
    "- **Support:** Number of samples in each class in the test set\n",
    "\n",
    "In real submarine warfare, you'd probably want to maximize recall for mines (catch every mine, even at the cost of some false alarms).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and ROC curve for best model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Mine', 'Rock'], yticklabels=['Mine', 'Rock'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Confusion Matrix: {best_model_name}')\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_best)\n",
    "auc = roc_auc_score(y_test, y_prob_best)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.3)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title(f'ROC Curve: {best_model_name}')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Mines correctly identified:  {cm[0,0]}\")\n",
    "print(f\"  True Rocks correctly identified:  {cm[1,1]}\")\n",
    "print(f\"  Mines misclassified as Rocks:     {cm[0,1]} <- DANGEROUS!\")\n",
    "print(f\"  Rocks misclassified as Mines:     {cm[1,0]} <- False alarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the confusion matrix:**\n",
    "\n",
    "- **Top-left (True Mines as Mines):** Good catches\n",
    "- **Bottom-right (True Rocks as Rocks):** Also good\n",
    "- **Top-right (Mines as Rocks):** THE DANGEROUS ONES - missed mines\n",
    "- **Bottom-left (Rocks as Mines):** False alarms - annoying but safe\n",
    "\n",
    "**ROC Curve:** Shows the trade-off between catching mines (True Positive Rate) and false alarms (False Positive Rate). The closer to the top-left corner, the better. AUC = 1.0 would be perfect.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance = best_model.feature_importances_\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': freq_cols,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 12))\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feat_imp)))\n",
    "    ax.barh(feat_imp['Feature'], feat_imp['Importance'], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(f'Feature Importance: {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Frequencies:\")\n",
    "    for _, row in feat_imp.tail(10).iloc[::-1].iterrows():\n",
    "        print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n{best_model_name} doesn't provide feature importances directly.\")\n",
    "    print(\"Using Random Forest for feature importance analysis...\")\n",
    "    \n",
    "    rf = models['Random Forest']\n",
    "    importance = rf.feature_importances_\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': freq_cols,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 12))\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feat_imp)))\n",
    "    ax.barh(feat_imp['Feature'], feat_imp['Importance'], color=colors, edgecolor='black')\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Feature Importance (Random Forest)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Frequencies (per Random Forest):\")\n",
    "    for _, row in feat_imp.tail(10).iloc[::-1].iterrows():\n",
    "        print(f\"  {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature importance tells us:** Which frequency bands are actually doing the heavy lifting in classification. The model learned to focus on certain frequencies - these are where the mine/rock signatures differ most.\n",
    "\n",
    "This connects back to our statistical analysis - the important features here should roughly match the ones with high Cohen's d values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Conclusion\n",
    "\n",
    "Time to wrap this up. What did we learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                      FINAL MISSION REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "DATASET SUMMARY:\n",
    "----------------\n",
    "- 208 sonar returns (111 mines, 97 rocks)\n",
    "- 60 frequency band measurements per sample\n",
    "- Signals from varying viewing angles\n",
    "- Classic benchmark dataset since 1988\n",
    "\n",
    "EDA FINDINGS:\n",
    "-------------\n",
    "- Classes are fairly balanced (53% mines, 47% rocks)\n",
    "- No missing values or duplicates\n",
    "- Mines and rocks have different frequency signatures\n",
    "- Middle frequency bands show strongest discrimination\n",
    "- Adjacent frequencies are highly correlated (redundancy exists)\n",
    "\n",
    "STATISTICAL INSIGHTS:\n",
    "---------------------\"\"\")\n",
    "\n",
    "sig_features = (stat_df['T_PValue'] < 0.05).sum()\n",
    "large_effect = (abs(stat_df['Cohens_d']) > 0.5).sum()\n",
    "print(f\"- {sig_features}/60 features statistically significant (p < 0.05)\")\n",
    "print(f\"- {large_effect}/60 features with medium+ effect size (|d| > 0.5)\")\n",
    "print(f\"- Top discriminator: {stat_df.iloc[0]['Feature']}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "DIMENSIONALITY REDUCTION:\n",
    "-------------------------\n",
    "- First {n_90} PCA components capture 90% of variance\n",
    "- Significant redundancy in the 60 features\n",
    "- 2D projections show overlapping classes (hard problem!)\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL PERFORMANCE:\n",
    "------------------\"\"\")\n",
    "print(f\"- Best model: {best_model_name}\")\n",
    "print(f\"- Test accuracy: {results_df.iloc[0]['Test_Accuracy']*100:.2f}%\")\n",
    "print(f\"- Cross-validation: {results_df.iloc[0]['CV_Mean']*100:.2f}% (+/- {results_df.iloc[0]['CV_Std']*100:.2f}%)\")\n",
    "print(f\"- ROC-AUC: {auc:.3f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "KEY TAKEAWAYS:\n",
    "--------------\n",
    "1. This is a genuinely hard classification problem\n",
    "2. No model achieves 100% - the classes overlap in feature space\n",
    "3. Ensemble methods and SVMs tend to perform best\n",
    "4. Middle frequency bands are most informative\n",
    "5. Cross-validation is essential with small datasets\n",
    "\n",
    "REAL-WORLD IMPLICATIONS:\n",
    "------------------------\n",
    "With {results_df.iloc[0]['Test_Accuracy']*100:.0f}% accuracy, roughly 1 in {int(1/(1-results_df.iloc[0]['Test_Accuracy']))} sonar \n",
    "returns would be misclassified. In submarine warfare, that's \n",
    "still risky - but way better than random guessing (50%).\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"                         MISSION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Bottom Line\n",
    "\n",
    "We took 208 sonar pings, analyzed 60 frequency bands, ran statistical tests, compressed dimensions, and threw 9 different algorithms at the problem.\n",
    "\n",
    "**What we learned:**\n",
    "- Mines and rocks DO have different sonar signatures\n",
    "- But the difference is subtle - not a clean separation\n",
    "- Machine learning can detect patterns humans would miss\n",
    "- Even the best models aren't perfect on this data\n",
    "\n",
    "**Why this matters:**\n",
    "This isn't just an academic exercise. Sonar classification is used in mine detection, submarine navigation, and underwater exploration. The techniques we used here - statistical analysis, dimensionality reduction, model comparison - are the same ones used in real-world signal processing.\n",
    "\n",
    "The dataset is small by modern standards, but it's been a benchmark for decades because it captures a fundamental challenge: extracting meaningful patterns from noisy, overlapping signals.\n",
    "\n",
    "**Next steps if you want to go further:**\n",
    "- Hyperparameter tuning (GridSearchCV)\n",
    "- Feature selection (remove redundant frequencies)\n",
    "- Ensemble stacking (combine multiple models)\n",
    "- Deep learning (though probably overkill for 208 samples)\n",
    "\n",
    "---\n",
    "\n",
    "*Stay safe out there. And if your sonar pings back something weird... maybe don't assume it's just a rock.*\n",
    "\n",
    "---\n",
    "\n",
    "**Connect:** [GitHub](https://github.com/Rekhii) | [Kaggle](https://kaggle.com/seki32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_extension": ".py",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
