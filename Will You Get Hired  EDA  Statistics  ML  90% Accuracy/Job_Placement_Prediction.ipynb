{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will You Get Hired? Let's Ask the Data\n",
    "\n",
    "## Job Placement Prediction | EDA | Statistics | ML\n",
    "\n",
    "---\n",
    "\n",
    "**Ah yes, the job hunt.** That beautiful period of life where you question every decision you've ever made while refreshing LinkedIn 47 times a day.\n",
    "\n",
    "But what if we could predict who gets placed and who doesn't? What actually matters - your grades? Your MBA score? That one internship you barely survived? Your ability to not panic during interviews?\n",
    "\n",
    "**This dataset has 215 candidates** with everything from their 10th grade scores to their interview performance. Some got placed. Some didn't. Let's figure out why.\n",
    "\n",
    "**What we're working with:**\n",
    "- Education history (SSC, HSC, Degree, MBA percentages)\n",
    "- Work experience & internships\n",
    "- Skills match, certifications\n",
    "- Interview scores\n",
    "- Company tier & competition level\n",
    "\n",
    "**The goal:** Build a model that predicts placement. And maybe learn what actually matters in this whole hiring game.\n",
    "\n",
    "Let's go.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                            accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, roc_curve)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Ready to predict some futures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard toolkit loaded. Nothing fancy - just the weapons every data scientist needs to answer life's big questions. Like \"why didn't they call me back?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Job_Placement_Data_Enhanced.csv')\n",
    "\n",
    "print(f\"Dataset: {df.shape[0]} candidates, {df.shape[1]} features\")\n",
    "print(f\"\\nFeatures: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**215 candidates, 20 features.** That's their entire professional existence reduced to 20 numbers and categories. Beautiful and slightly terrifying.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First look at our candidates.** SSC = 10th grade, HSC = 12th grade. We've got their entire academic journey plus work experience, interview scores, and whether they landed the job. \n",
    "\n",
    "Notice the mix - some high scorers got placed, but so did some average ones. Already interesting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: EDA - Who Gets Hired?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The big question: how many made it?\n",
    "status_counts = df['status'].value_counts()\n",
    "\n",
    "print(\"PLACEMENT RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Placed:     {status_counts['Placed']} ({status_counts['Placed']/len(df)*100:.1f}%)\")\n",
    "print(f\"Not Placed: {status_counts['Not Placed']} ({status_counts['Not Placed']/len(df)*100:.1f}%)\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(['Placed\\n(Got the job!)', 'Not Placed\\n(Back to LinkedIn)'], \n",
    "              [status_counts['Placed'], status_counts['Not Placed']], \n",
    "              color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Number of Candidates')\n",
    "ax.set_title('The Harsh Reality of Job Placement')\n",
    "for bar, count in zip(bars, [status_counts['Placed'], status_counts['Not Placed']]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "            str(count), ha='center', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**69% got placed, 31% didn't.** Not terrible odds, but that 31% is someone's sleepless nights and existential crisis. \n",
    "\n",
    "Also note: this is an imbalanced dataset. More placed than not. Our models might get lazy and just predict \"Placed\" for everyone. We'll watch for that.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data health check\n",
    "print(\"DATA HEALTH CHECK\")\n",
    "print(\"=\"*40)\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nNumerical features: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"Categorical features: {len(df.select_dtypes(include=['object']).columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean data.** No missing values, no duplicates. Someone did the preprocessing for us. Bless them.\n",
    "\n",
    "Mix of numerical (scores, percentages) and categorical (gender, board, degree type) features. We'll need to encode the categorical ones later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features summary\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"NUMERICAL FEATURES SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "df[num_cols].describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The numbers behind the candidates:**\n",
    "- SSC/HSC/Degree percentages: 40s to 90s range. Normal academic spread.\n",
    "- Interview scores: 50-98. Wide range - some crushed it, some choked.\n",
    "- Skills match: 40-98%. How well your skills fit the job.\n",
    "- Years experience: 0-5. Fresh grads to slightly seasoned folks.\n",
    "- Certifications: 0-5. The \"I took online courses\" metric.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features breakdown\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cat_cols.remove('status')  # That's our target\n",
    "\n",
    "print(\"CATEGORICAL FEATURES BREAKDOWN\")\n",
    "print(\"=\"*50)\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The categorical breakdown:**\n",
    "- Gender: More males in the dataset (typical in many fields)\n",
    "- Boards: Central vs Others (education board types)\n",
    "- Degrees: Commerce & Management dominates, followed by Sci&Tech\n",
    "- Specialization: Marketing & Finance vs Marketing & HR\n",
    "- Company tiers: MNC, Mid-tier, Startup\n",
    "- Competition: Low, Medium, High\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The money question: What's different between Placed and Not Placed?\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Key numerical features comparison\n",
    "key_features = ['mba_percent', 'interview_score', 'skills_match_percent', \n",
    "                'emp_test_percentage', 'degree_percentage', 'years_experience']\n",
    "\n",
    "for idx, col in enumerate(key_features):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    placed = df[df['status']=='Placed'][col]\n",
    "    not_placed = df[df['status']=='Not Placed'][col]\n",
    "    \n",
    "    ax.hist(placed, bins=15, alpha=0.6, color='green', label='Placed', edgecolor='black')\n",
    "    ax.hist(not_placed, bins=15, alpha=0.6, color='red', label='Not Placed', edgecolor='black')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{col} Distribution')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Placed vs Not Placed: Key Features Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting patterns emerging:**\n",
    "- **MBA percent:** Placed folks tend to score higher. Makes sense.\n",
    "- **Interview score:** This one's important - placed candidates cluster higher.\n",
    "- **Skills match:** Higher match = better chances. Duh.\n",
    "- **Years experience:** Surprisingly, not a huge separator.\n",
    "- **Degree percentage:** Some overlap, but placed folks skew higher.\n",
    "\n",
    "**Early hypothesis:** Interview score and skills match might be key predictors.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs Placement\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "cat_features = ['gender', 'work_experience', 'specialisation', \n",
    "                'company_tier', 'job_competition_level', 'undergrad_degree']\n",
    "\n",
    "for idx, col in enumerate(cat_features):\n",
    "    ax = axes[idx//3, idx%3]\n",
    "    cross = pd.crosstab(df[col], df['status'], normalize='index') * 100\n",
    "    cross.plot(kind='bar', stacked=True, color=['#e74c3c', '#2ecc71'], ax=ax, edgecolor='black')\n",
    "    ax.set_title(f'Placement Rate by {col}')\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.legend(['Not Placed', 'Placed'], loc='upper right')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.suptitle('How Categories Affect Your Chances', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Category insights:**\n",
    "- **Gender:** Pretty similar rates. Good - no obvious bias here.\n",
    "- **Work experience:** Having experience helps! (shocking, I know)\n",
    "- **Specialization:** Mkt&Fin slightly edges out Mkt&HR\n",
    "- **Company tier:** MNCs place better than startups? Interesting.\n",
    "- **Competition level:** High competition = fewer placements. Math checks out.\n",
    "- **Degree:** Sci&Tech and Comm&Mgmt have similar rates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical features\n",
    "num_cols_for_corr = ['ssc_percentage', 'hsc_percentage', 'degree_percentage', \n",
    "                     'emp_test_percentage', 'mba_percent', 'years_experience',\n",
    "                     'skills_match_percent', 'num_certifications', 'interview_score']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df[num_cols_for_corr].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, cmap='RdBu_r', center=0, \n",
    "            fmt='.2f', ax=ax, square=True)\n",
    "ax.set_title('Feature Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation insights:**\n",
    "- Academic scores correlate with each other (good student = consistently good)\n",
    "- Interview score shows some independence - you can be book smart but interview poorly\n",
    "- No extremely high correlations - features are reasonably independent\n",
    "\n",
    "Good news: No major multicollinearity issues to worry about.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Statistical Study\n",
    "\n",
    "Let's get serious. Which features ACTUALLY matter statistically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-tests for numerical features\n",
    "print(\"STATISTICAL SIGNIFICANCE: NUMERICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nH0: No difference between Placed and Not Placed\")\n",
    "print(\"H1: Significant difference exists\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "placed = df[df['status'] == 'Placed']\n",
    "not_placed = df[df['status'] == 'Not Placed']\n",
    "\n",
    "stat_results = []\n",
    "for col in num_cols_for_corr:\n",
    "    t_stat, p_val = stats.ttest_ind(placed[col], not_placed[col])\n",
    "    \n",
    "    # Cohen's d\n",
    "    pooled_std = np.sqrt((placed[col].std()**2 + not_placed[col].std()**2) / 2)\n",
    "    cohens_d = (placed[col].mean() - not_placed[col].mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    stat_results.append({\n",
    "        'Feature': col,\n",
    "        'Placed_Mean': placed[col].mean(),\n",
    "        'NotPlaced_Mean': not_placed[col].mean(),\n",
    "        'Difference': placed[col].mean() - not_placed[col].mean(),\n",
    "        'T_Stat': t_stat,\n",
    "        'P_Value': p_val,\n",
    "        'Cohens_d': cohens_d,\n",
    "        'Significant': 'YES' if p_val < 0.05 else 'NO'\n",
    "    })\n",
    "\n",
    "stat_df = pd.DataFrame(stat_results).sort_values('P_Value')\n",
    "\n",
    "for _, row in stat_df.iterrows():\n",
    "    sig = \"***\" if row['Significant'] == 'YES' else \"   \"\n",
    "    print(f\"{sig} {row['Feature']:25s} | p={row['P_Value']:.4f} | d={row['Cohens_d']:+.3f} | {row['Significant']}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nSignificant features: {(stat_df['Significant']=='YES').sum()}/{len(stat_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical verdict on numerical features:**\n",
    "- **p < 0.05** = statistically significant difference between placed and not placed\n",
    "- **Cohen's d** = effect size (positive means placed folks score higher)\n",
    "\n",
    "The significant features are our best predictors. Non-significant ones? They don't reliably distinguish the two groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square tests for categorical features\n",
    "print(\"STATISTICAL SIGNIFICANCE: CATEGORICAL FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nChi-Square Tests for Independence\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "chi_results = []\n",
    "for col in cat_cols:\n",
    "    contingency = pd.crosstab(df[col], df['status'])\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(contingency)\n",
    "    \n",
    "    # Cramer's V\n",
    "    n = contingency.sum().sum()\n",
    "    min_dim = min(contingency.shape) - 1\n",
    "    cramers_v = np.sqrt(chi2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "    \n",
    "    chi_results.append({\n",
    "        'Feature': col,\n",
    "        'Chi2': chi2,\n",
    "        'P_Value': p_val,\n",
    "        'Cramers_V': cramers_v,\n",
    "        'Significant': 'YES' if p_val < 0.05 else 'NO'\n",
    "    })\n",
    "\n",
    "chi_df = pd.DataFrame(chi_results).sort_values('P_Value')\n",
    "\n",
    "for _, row in chi_df.iterrows():\n",
    "    sig = \"***\" if row['Significant'] == 'YES' else \"   \"\n",
    "    print(f\"{sig} {row['Feature']:25s} | p={row['P_Value']:.4f} | V={row['Cramers_V']:.3f} | {row['Significant']}\")\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nSignificant features: {(chi_df['Significant']=='YES').sum()}/{len(chi_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chi-square results:**\n",
    "- Tests if categorical features are independent of placement status\n",
    "- **CramÃ©r's V** = strength of association (0 = none, 1 = perfect)\n",
    "\n",
    "Significant categorical features actually influence whether you get placed. Non-significant ones? Random noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect sizes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Cohen's d for numerical\n",
    "stat_df_sorted = stat_df.sort_values('Cohens_d')\n",
    "colors = ['#2ecc71' if d > 0 else '#e74c3c' for d in stat_df_sorted['Cohens_d']]\n",
    "axes[0].barh(stat_df_sorted['Feature'], stat_df_sorted['Cohens_d'], color=colors, edgecolor='black')\n",
    "axes[0].axvline(x=0, color='black', linewidth=1)\n",
    "axes[0].axvline(x=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium effect')\n",
    "axes[0].axvline(x=-0.5, color='orange', linestyle='--', alpha=0.7)\n",
    "axes[0].set_xlabel(\"Cohen's d\")\n",
    "axes[0].set_title('Effect Size: Numerical Features\\n(Green = Placed higher, Red = Not Placed higher)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cramer's V for categorical\n",
    "chi_df_sorted = chi_df.sort_values('Cramers_V')\n",
    "colors_v = ['#2ecc71' if row['Significant']=='YES' else '#95a5a6' for _, row in chi_df_sorted.iterrows()]\n",
    "axes[1].barh(chi_df_sorted['Feature'], chi_df_sorted['Cramers_V'], color=colors_v, edgecolor='black')\n",
    "axes[1].axvline(x=0.1, color='orange', linestyle='--', alpha=0.7, label='Small effect (0.1)')\n",
    "axes[1].set_xlabel(\"Cramer's V\")\n",
    "axes[1].set_title('Effect Size: Categorical Features\\n(Green = Significant)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effect size visualization:**\n",
    "- Bigger bars = stronger relationship with placement\n",
    "- Green numerical features = placed candidates score higher\n",
    "- These are the features our models should lean on\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model Building\n",
    "\n",
    "Time to see if machines can predict who gets hired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df_model = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_model[col] = le.fit_transform(df_model[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "df_model['status'] = le_target.fit_transform(df_model['status'])\n",
    "print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Features and target\n",
    "X = df_model.drop('status', axis=1)\n",
    "y = df_model['status']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data prep done:**\n",
    "- Categorical features encoded to numbers\n",
    "- 80/20 train/test split\n",
    "- Stratified to maintain class balance\n",
    "- Scaled for algorithms that need it\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model battle\n",
    "models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000, random_state=42), True),\n",
    "    'Decision Tree': (DecisionTreeClassifier(random_state=42), False),\n",
    "    'Random Forest': (RandomForestClassifier(n_estimators=100, random_state=42), False),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(random_state=42), False),\n",
    "    'K-Nearest Neighbors': (KNeighborsClassifier(n_neighbors=5), True),\n",
    "    'SVM': (SVC(kernel='rbf', probability=True, random_state=42), True),\n",
    "    'Naive Bayes': (GaussianNB(), True)\n",
    "}\n",
    "\n",
    "results = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"MODEL BATTLE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<25} | {'Test Acc':>10} | {'CV Mean':>10} | {'F1':>8} | {'ROC-AUC':>8}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, (model, use_scaled) in models.items():\n",
    "    X_tr = X_train_scaled if use_scaled else X_train\n",
    "    X_te = X_test_scaled if use_scaled else X_test\n",
    "    \n",
    "    model.fit(X_tr, y_train)\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_prob = model.predict_proba(X_te)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc = roc_auc_score(y_test, y_prob) if y_prob is not None else 0\n",
    "    \n",
    "    cv_X = X_train_scaled if use_scaled else X_train\n",
    "    cv_scores = cross_val_score(model, cv_X, y_train, cv=cv)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Test_Acc': acc,\n",
    "        'CV_Mean': cv_scores.mean(),\n",
    "        'CV_Std': cv_scores.std(),\n",
    "        'F1': f1,\n",
    "        'ROC_AUC': roc\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:<25} | {acc*100:>9.2f}% | {cv_scores.mean()*100:>9.2f}% | {f1:>7.3f} | {roc:>7.3f}\")\n",
    "\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model comparison:**\n",
    "- **Test Acc:** Performance on held-out data\n",
    "- **CV Mean:** 5-fold cross-validation (more reliable)\n",
    "- **F1:** Balance of precision and recall\n",
    "- **ROC-AUC:** Overall discrimination ability\n",
    "\n",
    "Higher is better across all metrics. Let's see who won.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaderboard\n",
    "results_df = pd.DataFrame(results).sort_values('Test_Acc', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL LEADERBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for rank, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    medal = \"CHAMPION\" if rank == 1 else f\"#{rank}\"\n",
    "    print(f\"[{medal:^8}] {row['Model']:<25} | {row['Test_Acc']*100:.2f}%\")\n",
    "\n",
    "champion = results_df.iloc[0]\n",
    "print(f\"\\nWINNER: {champion['Model']}\")\n",
    "print(f\"Test Accuracy: {champion['Test_Acc']*100:.2f}%\")\n",
    "print(f\"ROC-AUC: {champion['ROC_AUC']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have a winner!** The best model for predicting job placement has been crowned.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "colors = ['gold' if acc == results_df['Test_Acc'].max() else 'steelblue' for acc in results_df['Test_Acc']]\n",
    "axes[0].barh(results_df['Model'], results_df['Test_Acc'], color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Test Accuracy')\n",
    "axes[0].set_xlim(0.6, 1.0)\n",
    "for i, (_, row) in enumerate(results_df.iterrows()):\n",
    "    axes[0].text(row['Test_Acc'] + 0.01, i, f\"{row['Test_Acc']*100:.1f}%\", va='center', fontweight='bold')\n",
    "\n",
    "# CV with error bars\n",
    "axes[1].barh(results_df['Model'], results_df['CV_Mean'], \n",
    "             xerr=results_df['CV_Std'], color='steelblue', edgecolor='black', capsize=5)\n",
    "axes[1].set_xlabel('Accuracy')\n",
    "axes[1].set_title('Cross-Validation Accuracy (with std)')\n",
    "axes[1].set_xlim(0.6, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual leaderboard:**\n",
    "- Gold bar = champion\n",
    "- Error bars show stability across CV folds\n",
    "- Smaller error bars = more reliable performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model deep dive\n",
    "best_name = results_df.iloc[0]['Model']\n",
    "best_model = models[best_name][0]\n",
    "use_scaled = models[best_name][1]\n",
    "\n",
    "X_te_final = X_test_scaled if use_scaled else X_test\n",
    "y_pred_final = best_model.predict(X_te_final)\n",
    "y_prob_final = best_model.predict_proba(X_te_final)[:, 1]\n",
    "\n",
    "print(f\"CHAMPION MODEL: {best_name}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=['Not Placed', 'Placed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification report breakdown:**\n",
    "- **Precision for Placed:** When we predict \"Placed\", how often are we right?\n",
    "- **Recall for Placed:** Of everyone who actually got placed, how many did we catch?\n",
    "- **Precision for Not Placed:** When we predict rejection, how often are we right?\n",
    "- **Recall for Not Placed:** Of everyone rejected, how many did we identify?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and ROC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Not Placed', 'Placed'], \n",
    "            yticklabels=['Not Placed', 'Placed'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title(f'Confusion Matrix: {best_name}')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_final)\n",
    "auc = roc_auc_score(y_test, y_prob_final)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "axes[1].fill_between(fpr, tpr, alpha=0.3)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  Correctly predicted NOT placed: {cm[0,0]}\")\n",
    "print(f\"  Correctly predicted placed:     {cm[1,1]}\")\n",
    "print(f\"  Missed placements (False Neg):  {cm[1,0]}\")\n",
    "print(f\"  False hopes (False Pos):        {cm[0,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The verdict:**\n",
    "- **True Negatives:** Correctly told people they wouldn't get placed\n",
    "- **True Positives:** Correctly predicted placements\n",
    "- **False Negatives:** Said \"no\" to people who actually got placed (sad)\n",
    "- **False Positives:** Gave false hope (also sad, differently)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "else:\n",
    "    # Use Random Forest for feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    feat_imp = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feat_imp)))\n",
    "ax.barh(feat_imp['Feature'], feat_imp['Importance'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('What Actually Matters for Getting Hired?')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTOP 5 FEATURES FOR PLACEMENT:\")\n",
    "for _, row in feat_imp.tail(5).iloc[::-1].iterrows():\n",
    "    print(f\"  {row['Feature']:25s}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The hiring cheat code (sort of):**\n",
    "\n",
    "These are the features that matter most according to the model. If you want to get placed, focus on the top ones. Or don't - I'm a notebook, not your career counselor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Conclusion\n",
    "\n",
    "What did we learn about getting hired?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                         FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATASET:\n",
    "--------\n",
    "- 215 candidates, 19 features\n",
    "- Placed: {status_counts['Placed']} ({status_counts['Placed']/len(df)*100:.1f}%) | Not Placed: {status_counts['Not Placed']} ({status_counts['Not Placed']/len(df)*100:.1f}%)\n",
    "\n",
    "STATISTICAL INSIGHTS:\n",
    "--------------------\n",
    "- {(stat_df['Significant']=='YES').sum()}/{len(stat_df)} numerical features significantly different between groups\n",
    "- {(chi_df['Significant']=='YES').sum()}/{len(chi_df)} categorical features significantly associated with placement\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "-----------------\n",
    "- Champion: {champion['Model']}\n",
    "- Test Accuracy: {champion['Test_Acc']*100:.2f}%\n",
    "- ROC-AUC: {champion['ROC_AUC']:.3f}\n",
    "\n",
    "KEY FINDINGS:\n",
    "------------\"\"\")\n",
    "\n",
    "print(f\"Top 3 important features: {', '.join(feat_imp.tail(3)['Feature'].tolist()[::-1])}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "REAL TALK:\n",
    "---------\n",
    "- No single feature guarantees placement\n",
    "- It's a combination of academics, skills, and interview performance\n",
    "- Work experience helps but isn't everything\n",
    "- The model is ~{champion['Test_Acc']*100:.0f}% accurate - not perfect, but useful\n",
    "\n",
    "Remember: This is data, not destiny. Models can predict probabilities,\n",
    "not determine your future. Keep grinding.\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"                      GOOD LUCK OUT THERE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Bottom Line\n",
    "\n",
    "We took 215 candidates, analyzed their entire academic and professional history, ran statistical tests, and threw 7 machine learning algorithms at the problem.\n",
    "\n",
    "**What actually matters for getting placed:**\n",
    "- Your academic scores DO matter (but aren't everything)\n",
    "- Interview performance is crucial\n",
    "- Skills match and relevant experience help\n",
    "- It's a combination, not a single magic number\n",
    "\n",
    "**What the model achieved:**\n",
    "- Solid prediction accuracy\n",
    "- Can identify likely placements with reasonable confidence\n",
    "- Not perfect - humans are messy and unpredictable\n",
    "\n",
    "**Final thought:** \n",
    "Machine learning can find patterns in placement data, but it can't capture everything that makes someone hireable - your energy in the interview, your ability to connect with people, that random project you did that perfectly matched what the company needed.\n",
    "\n",
    "Data is a guide, not a guarantee. Use it wisely.\n",
    "\n",
    "---\n",
    "\n",
    "**Connect:** [GitHub](https://github.com/Rekhii) | [Kaggle](https://kaggle.com/seki32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
